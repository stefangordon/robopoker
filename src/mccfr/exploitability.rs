//! Exploitability evaluation using Monte Carlo best response sampling.
//!
//! Measures how much a strategy can be exploited by computing the Nash gap:
//! the difference between best response and strategy values. Uses statistical
//! stopping with confidence intervals for efficient evaluation.

use crate::{
    cards::{hand::Hand, street::Street, evaluator::Evaluator},
    mccfr::{
        nlhe::{encoder::Encoder, game::Game, solver::NLHE, turn::Turn, edge::Edge, info::Info},
        structs::tree::Tree,
        traits::{blueprint::Blueprint, encoder::Encoder as EncoderTrait, game::Game as GameTrait, profile::Profile},
    },
    save::disk::Disk,
    Arbitrary, B_BLIND,
};

use rand::{prelude::*, rngs::SmallRng, SeedableRng};
use rayon::prelude::*;
use std::{cmp::Ordering, time::{Duration, Instant}};
use petgraph::graph::NodeIndex;

// Evaluation configuration
const ROLLOUT_COUNT: usize = 5000;

/// Parallelization batch size - defaults to number of CPUs
fn batch_size() -> usize {
    num_cpus::get()
}

const MAX_DEPTH: usize = 20;
const TARGET_CI_WIDTH: f64 = 5.0;  // ±5 mbb/hand
const MIN_SAMPLES: u64 = 100;
const PROGRESS_DELAY_MS: u64 = 100;

/// Runs exploitability evaluation with statistical stopping.
pub async fn evaluate() {
    let solver = load_solver().await;
    let mut stats = WelfordStats::new();
    let start_time = Instant::now();

    log::info!("Starting exploitability evaluation with exact equity calculation (no bias)");
    log::info!("Target confidence interval: ±{TARGET_CI_WIDTH} mbb/hand");

    loop {
        let batch_results = evaluate_batch(&solver).await;

        batch_results.iter().for_each(|&result| stats.update(result));
        let samples = stats.count();

        let (mean, ci_width) = stats.confidence_interval();
        let rate = samples as f64 / start_time.elapsed().as_secs_f64();

        log::info!("Samples: {samples} | Exploitability: {mean:.2} ± {ci_width:.2} mbb/hand | Rate: {rate:.1}/sec");

        if samples >= MIN_SAMPLES && ci_width <= TARGET_CI_WIDTH {
            log::info!("✓ Converged: {mean:.2} mbb/hand");
            break;
        }

        tokio::time::sleep(Duration::from_millis(PROGRESS_DELAY_MS)).await;
    }
}

async fn load_solver() -> NLHE {
    log::info!("Loading trained solver...");
    NLHE::load(Street::random())
}

async fn evaluate_batch(solver: &NLHE) -> Vec<f64> {
    (0..batch_size())
        .into_par_iter()
        .map(|_| {
            let mut rng = SmallRng::from_entropy();
            let game = Game::root();
            compute_exploitability(&game, solver.profile(), solver.encoder(), &mut rng)
        })
        .collect()
}

/// Computes exploitability for a single game using Monte Carlo rollouts.
fn compute_exploitability(
    game: &Game,
    profile: &super::nlhe::profile::Profile,
    encoder: &Encoder,
    rng: &mut SmallRng,
) -> f64 {
    // Reuse trees for efficiency - clear between rollouts
    let mut br_tree = Tree::default();
    let mut strategy_tree = Tree::default();

    (0..ROLLOUT_COUNT)
        .map(|_| {
            // Clear trees for reuse
            br_tree.clear();
            strategy_tree.clear();

            let br_values = [0, 1].map(|player|
                best_response_value(game, profile, encoder, &mut br_tree, player, MAX_DEPTH, rng)
            );

            let strategy_values = [0, 1].map(|player|
                strategy_value(game, profile, encoder, &mut strategy_tree, player, MAX_DEPTH, rng)
            );

            let exploitability = br_values.iter().zip(&strategy_values)
                .map(|(br, strat)| br - strat)
                .sum::<f64>() / 2.0; // Standard formula

            exploitability * 1000.0 / B_BLIND as f64 // Convert to mbb/hand
        })
        .sum::<f64>() / ROLLOUT_COUNT as f64
}

/// Computes best response value using perfect play for the target player.
fn best_response_value(
    game: &Game,
    profile: &super::nlhe::profile::Profile,
    encoder: &Encoder,
    tree: &mut Tree<Turn, Edge, Game, Info>,
    player: usize,
    depth: usize,
    rng: &mut SmallRng,
) -> f64 {
    // Seed root node
    let root_info = encoder.seed(game);
    let root_node = tree.seed(root_info, game.clone());
    best_response_node_value(root_node.index(), profile, encoder, tree, player, depth, 1.0, rng)
}

/// Recursive helper that traverses the tree using encoder.info()
fn best_response_node_value(
    node_idx: NodeIndex,
    profile: &super::nlhe::profile::Profile,
    encoder: &Encoder,
    tree: &mut Tree<Turn, Edge, Game, Info>,
    player: usize,
    depth: usize,
    reach: f64,
    _rng: &mut SmallRng,
) -> f64 {
    let node = tree.at(node_idx);
    let game = node.game();

    if depth == 0 {
        log::trace!("BR depth limited at street {:?}, pot={}, turn={:?}",
                   game.street(), game.pot(), game.turn());
        return reach * estimate_leaf_value(game, player);
    }

    // Generate branches (legal edges) from this node
    let branches = encoder.branches(&node);
    if branches.is_empty() {
        return reach * game.payoff(Turn::Choice(player)) as f64;
    }

    match game.turn() {
        Turn::Chance => {
            let prob = 1.0 / branches.len() as f64;
            branches
                .iter()
                .map(|(edge, next_game, _)| {
                    let child_info = encoder.info(tree, (edge.clone(), next_game.clone(), node_idx));
                    let child_node = tree.grow(child_info, (edge.clone(), next_game.clone(), node_idx));
                    best_response_node_value(child_node.index(), profile, encoder, tree, player, depth - 1, reach * prob, _rng)
                })
                .sum()
        },

        Turn::Choice(p) if p == player => {
            // Player chooses the action that maximises expected value.
            branches
                .iter()
                .map(|(edge, next_game, _)| {
                    let child_info = encoder.info(tree, (edge.clone(), next_game.clone(), node_idx));
                    let child_node = tree.grow(child_info, (edge.clone(), next_game.clone(), node_idx));
                    best_response_node_value(child_node.index(), profile, encoder, tree, player, depth - 1, reach, _rng)
                })
                .fold(f64::NEG_INFINITY, f64::max)
        },

        Turn::Choice(_) => {
            // Opponent follows their strategy – take expectation over their mixed strategy.
            let probs = normalize_probabilities(&branches, profile, node.info());
            branches
                .iter()
                .zip(&probs)
                .map(|((edge, next_game, _), prob)| {
                    let child_info = encoder.info(tree, (edge.clone(), next_game.clone(), node_idx));
                    let child_node = tree.grow(child_info, (edge.clone(), next_game.clone(), node_idx));
                    best_response_node_value(child_node.index(), profile, encoder, tree, player, depth - 1, reach * *prob, _rng)
                })
                .sum()
        },

        Turn::Terminal => reach * game.payoff(Turn::Choice(player)) as f64,
    }
}

/// Computes strategy value using the current profile for both players.
fn strategy_value(
    game: &Game,
    profile: &super::nlhe::profile::Profile,
    encoder: &Encoder,
    tree: &mut Tree<Turn, Edge, Game, Info>,
    player: usize,
    depth: usize,
    rng: &mut SmallRng,
) -> f64 {
    let root_info = encoder.seed(game);
    let root_node = tree.seed(root_info, game.clone());
    strategy_node_value(root_node.index(), profile, encoder, tree, player, depth, 1.0, rng)
}

/// Recursive helper for average-strategy rollout
fn strategy_node_value(
    node_idx: NodeIndex,
    profile: &super::nlhe::profile::Profile,
    encoder: &Encoder,
    tree: &mut Tree<Turn, Edge, Game, Info>,
    player: usize,
    depth: usize,
    reach: f64,
    _rng: &mut SmallRng,
) -> f64 {
    let node = tree.at(node_idx);
    let game = node.game();

    if depth == 0 {
        log::warn!("Strategy depth limited at street {:?}, pot={}, turn={:?}",
                   game.street(), game.pot(), game.turn());
        return reach * estimate_leaf_value(game, player);
    }

    let branches = encoder.branches(&node);
    if branches.is_empty() {
        return reach * game.payoff(Turn::Choice(player)) as f64;
    }

    match game.turn() {
        Turn::Chance => {
            let prob = 1.0 / branches.len() as f64;
            branches
                .iter()
                .map(|(edge, next_game, _)| {
                    let child_info = encoder.info(tree, (edge.clone(), next_game.clone(), node_idx));
                    let child_node = tree.grow(child_info, (edge.clone(), next_game.clone(), node_idx));
                    strategy_node_value(child_node.index(), profile, encoder, tree, player, depth - 1, reach * prob, _rng)
                })
                .sum()
        },

        Turn::Choice(_) => {
            let probs = normalize_probabilities(&branches, profile, node.info());
            branches
                .iter()
                .zip(&probs)
                .map(|((edge, next_game, _), prob)| {
                    let child_info = encoder.info(tree, (edge.clone(), next_game.clone(), node_idx));
                    let child_node = tree.grow(child_info, (edge.clone(), next_game.clone(), node_idx));
                    strategy_node_value(child_node.index(), profile, encoder, tree, player, depth - 1, reach * *prob, _rng)
                })
                .sum()
        },

        Turn::Terminal => reach * game.payoff(Turn::Choice(player)) as f64,
    }
}

type TreeBranches = Vec<(Edge, Game, petgraph::graph::NodeIndex)>;

fn normalize_probabilities(
    branches: &TreeBranches,
    profile: &super::nlhe::profile::Profile,
    info: &Info,
) -> Vec<f64> {
    let mut probs: Vec<f64> = branches.iter()
        .map(|(edge, _, _)| profile.advice(info, edge) as f64)
        .collect();

    let sum: f64 = probs.iter().sum();
    if sum > 0.0 {
        probs.iter_mut().for_each(|p| *p /= sum);
    } else {
        let len = probs.len() as f64;
        probs.fill(1.0 / len);
    }
    probs
}

/// Estimates leaf node value using equity calculation.
fn estimate_leaf_value(game: &Game, player: usize) -> f64 {
    // Expected share of the pot minus chips already committed by the hero.
    // This avoids double-counting the hero's contribution and keeps the value
    // bounded within ±(pot / 2) chips.

    let pot = game.pot() as f64;
    let equity = compute_equity(game, player);

    // Chips hero has risked so far (part of the pot already).
    let committed = game.spent(player) as f64;

    equity * pot - committed
}

/// Computes hero's equity against the opponent using their actual hole cards
/// and exhaustive enumeration over the remaining community cards.
fn compute_equity(game: &Game, hero_player: usize) -> f64 {
    // Known hole cards for both players
    let hero_hole = Hand::from(game.hole_cards(hero_player));
    let villain_hole = Hand::from(game.hole_cards(1 - hero_player));

    let board = Hand::from(game.board());

    street_equity(hero_hole, villain_hole, board, game.street())
}

/// Exact equity calculation using exhaustive board enumeration where feasible.
fn street_equity(hero: Hand, villain: Hand, board: Hand, _street: Street) -> f64 {
    // Create full deck and remove known cards
    let mut remaining = Hand::from(Hand::mask()); // Full 52-card deck

    // Remove known cards
    for card in Vec::from(Hand::add(Hand::add(hero, villain), board)) {
        remaining.remove(card);
    }

    let cards_needed = 5_usize.saturating_sub(board.size());

    match cards_needed {
        0 => {
            // Board is complete - direct evaluation
            evaluate_completed_hands(hero, villain, board)
        },

        1 => {
            // Turn to river - enumerate all ~46 possibilities (exact)
            let mut sum = 0.0;
            let mut total = 0u32;

            // Use Hand's native iterator - no Vec allocation
            for river_card in remaining {
                let final_board = Hand::add(board, Hand::from(river_card));
                let result = evaluate_completed_hands(hero, villain, final_board);

                sum += result;
                total += 1;
            }

            if total > 0 {
                sum / total as f64
            } else {
                0.5
            }
        },

        2 => {
            // Flop to turn-river – enumerate all 2-card combinations (C(n,2) ≤ 1 k).
            let remaining_cards: Vec<_> = remaining.collect();
            let mut sum = 0.0;
            let mut total = 0u32;

            for i in 0..remaining_cards.len() {
                for j in (i + 1)..remaining_cards.len() {
                    let turn_board = Hand::add(board, Hand::from(remaining_cards[i]));
                    let final_board = Hand::add(turn_board, Hand::from(remaining_cards[j]));
                    sum += evaluate_completed_hands(hero, villain, final_board);
                    total += 1;
                }
            }

            sum / total as f64
        },

        3 => {
            // Pre-flop to flop-turn-river – enumerate all 3-card combinations (C(n,3) ≤ 17 k).
            let remaining_cards: Vec<_> = remaining.collect();
            let mut sum = 0.0;
            let mut total = 0u32;

            for i in 0..remaining_cards.len() {
                for j in (i + 1)..remaining_cards.len() {
                    for k in (j + 1)..remaining_cards.len() {
                        let flop = Hand::add(board, Hand::from(remaining_cards[i]));
                        let flop = Hand::add(flop, Hand::from(remaining_cards[j]));
                        let flop = Hand::add(flop, Hand::from(remaining_cards[k]));
                        sum += evaluate_completed_hands(hero, villain, flop);
                        total += 1;
                    }
                }
            }

            sum / total as f64
        },

        4 | 5 => {
            // Exact enumeration explodes (≈180 k-1.2 M combos). For exploitability evaluation
            // this is still tractable but we guard with a fallback if depth-limited leaves
            // make this hot.  We use reservoir-sampling when combinations exceed 200k.

            const MAX_EXACT: usize = 200_000; // Rough guard

            let remaining_cards: Vec<_> = remaining.collect();
            fn n_choose_k(n: usize, k: usize) -> usize {
                if k > n { return 0; }
                let mut res = 1usize;
                for i in 0..k {
                    res = res * (n - i) / (i + 1);
                }
                res
            }

            let combos = n_choose_k(remaining_cards.len(), cards_needed);

            if combos <= MAX_EXACT {
                // Exhaustive enumeration using indices vector
                let mut indices = (0..cards_needed).collect::<Vec<usize>>();
                let mut sum = 0.0f64;
                let mut count = 0usize;

                loop {
                    // Build new board
                    let mut extended_board = board;
                    for &idx in &indices {
                        extended_board = Hand::add(extended_board, Hand::from(remaining_cards[idx]));
                    }
                    sum += evaluate_completed_hands(hero, villain, extended_board);
                    count += 1;

                    // Next combination (lexicographic)
                    let mut i = cards_needed;
                    while i > 0 {
                        i -= 1;
                        if indices[i] != i + remaining_cards.len() - cards_needed {
                            break;
                        }
                    }
                    if i == 0 && indices[0] == remaining_cards.len() - cards_needed {
                        break; // Finished
                    }
                    indices[i] += 1;
                    for j in i + 1..cards_needed {
                        indices[j] = indices[j - 1] + 1;
                    }
                }

                sum / count as f64
            } else {
                // Too many – fall back to Monte Carlo with 10k samples (still unbiased)
                let mut rng = rand::thread_rng();
                let mut sum = 0.0;
                for _ in 0..10_000 {
                    let mut sampled = rand::seq::index::sample(&mut rng, remaining_cards.len(), cards_needed).into_vec();
                    sampled.sort_unstable();
                    let mut extended_board = board;
                    for idx in sampled {
                        extended_board = Hand::add(extended_board, Hand::from(remaining_cards[idx]));
                    }
                    sum += evaluate_completed_hands(hero, villain, extended_board);
                }
                sum / 10_000.0
            }
        },

        _ => unreachable!("cards_needed should be 0..=5, got {}", cards_needed)
    }
}

/// Evaluate two hands against a completed 5-card board.
fn evaluate_completed_hands(hero: Hand, villain: Hand, board: Hand) -> f64 {
    let hero_hand = Hand::add(hero, board);
    let villain_hand = Hand::add(villain, board);

    let hero_eval = Evaluator::from(hero_hand);
    let villain_eval = Evaluator::from(villain_hand);
    let hero_ranking = hero_eval.find_ranking();
    let villain_ranking = villain_eval.find_ranking();

    let result = match hero_ranking.cmp(&villain_ranking) {
        Ordering::Greater => 1.0,
        Ordering::Equal => 0.5,
        Ordering::Less => 0.0,
    };

    // Debug logging to verify correctness (can be removed in production)
    #[cfg(debug_assertions)]
    if cfg!(test) {
        println!("Hero: {} -> {:?}", hero_hand, hero_ranking);
        println!("Villain: {} -> {:?}", villain_hand, villain_ranking);
        println!("Result: {} (hero wins: {})", result, result > 0.5);
    }

    result
}

/// Welford's online algorithm for computing running statistics.
#[derive(Debug, Clone)]
struct WelfordStats {
    count: u64,
    mean: f64,
    m2: f64, // Sum of squared deviations
}

impl WelfordStats {
    const fn new() -> Self {
        Self { count: 0, mean: 0.0, m2: 0.0 }
    }

    fn update(&mut self, value: f64) {
        self.count += 1;
        let delta = value - self.mean;
        self.mean += delta / self.count as f64;
        self.m2 += delta * (value - self.mean);
    }

    const fn count(&self) -> u64 { self.count }

    fn std_error(&self) -> f64 {
        if self.count < 2 { return f64::INFINITY; }
        let variance = self.m2 / (self.count - 1) as f64;
        (variance / self.count as f64).sqrt()
    }

    /// Returns (mean, 95% CI width).
    fn confidence_interval(&self) -> (f64, f64) {
        (self.mean, 1.96 * self.std_error())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::cards::ranking::Ranking;
    use crate::cards::rank::Rank;

    #[test]
    fn test_equity_logic_with_known_hands() {
        // Test a scenario where we know the outcome
        // Royal flush vs high card should be 100% equity

        // We'll create hands manually to test the core logic
        let royal_flush = Ranking::StraightFlush(Rank::Ace);
        let high_card = Ranking::HighCard(Rank::Ace);

        // Test the comparison directly
        let comparison = royal_flush.cmp(&high_card);

        match comparison {
            Ordering::Greater => {
                println!("✓ CORRECT: Royal flush > high card (Greater)");
                assert!(true);
            },
            Ordering::Less => {
                println!("❌ BUG: Royal flush < high card (Less) - COMPARISON IS BACKWARDS!");
                panic!("Ranking comparison is backwards! Royal flush should beat high card");
            },
            Ordering::Equal => {
                println!("❌ BUG: Royal flush = high card (Equal) - This shouldn't happen");
                panic!("Ranking comparison failed - royal flush equals high card");
            }
        }

        // Test the actual function that would be called
        let hero_wins_result = match royal_flush.cmp(&high_card) {
            Ordering::Greater => 1.0,  // Hero wins
            Ordering::Equal => 0.5,    // Tie
            Ordering::Less => 0.0,     // Hero loses
        };

        assert_eq!(hero_wins_result, 1.0, "Royal flush should beat high card (equity = 1.0)");
        println!("✓ Equity calculation logic is correct");
    }


}